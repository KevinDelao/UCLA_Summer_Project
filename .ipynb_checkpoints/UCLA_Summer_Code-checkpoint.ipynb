{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this file was mainly used to test out different things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shap\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import randrange\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# corr_list = []\n",
    "# for i in range(1,100):\n",
    "#     f_M = np.random.uniform(0,1,5)\n",
    "#     G = np.random.binomial(n=2,p = f_M, size = (100,5))\n",
    "# #     G = preprocessing.scale(G, axis=0)\n",
    "#     G = pd.DataFrame(G)\n",
    "#     corr  = G.corr()\n",
    "#     corr_list.append(corr)\n",
    "# with open('/Users/kevin/Downloads/Correlations_100', 'wb') as fp:\n",
    "#     pickle.dump(corr_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open ('/Users/kevin/Downloads/Correlations_100', 'rb') as fp:\n",
    "#     corr2list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trait_simulation_oneloci(samples_n, loci_m,var_g,var_e):\n",
    "    f_M = np.random.uniform(0,1,loci_m)\n",
    "    G = np.random.binomial(n=2,p = f_M, size = (samples_n,loci_m))\n",
    "    G = preprocessing.scale(G, axis=0)\n",
    "    loci =random.randint(0,loci_m-1)\n",
    "    print(loci)\n",
    "    SNP = G[:,loci]\n",
    "    individuals = len(SNP)\n",
    "    mean = 0 \n",
    "    sigma_b = sqrt(var_g)\n",
    "    sigma_e = sqrt(var_e)\n",
    "    b_i = np.random.normal(0, sigma_b)\n",
    "    Y_n = np.zeros((individuals, 1))\n",
    "    for k in range(0, individuals):\n",
    "        #each individual will have a random e_j(noise) value\n",
    "        e_j = np.random.normal(0, sigma_e)\n",
    "        #G_ij will be the jth individual from our SNP for the loci of choce\n",
    "        G_ij  = SNP[k]\n",
    "        Y_j = b_i*G_ij + e_j\n",
    "        Y_n[k] = Y_j \n",
    "    y_max = np.max(Y_n)\n",
    "    y_max = abs(y_max)\n",
    "    Y_n = np.array(Y_n)\n",
    "    Y_n = Y_n/y_max\n",
    "    Y_n = Y_n.reshape(samples_n,1) \n",
    "    H= var_g/(var_g+var_e)\n",
    "    G = np.append(G, Y_n, axis=1)\n",
    "    return G,samples_n,loci_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Loci Binary Traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trait_simulation_oneloci_binary(samples_n, loci_m,var_g,var_e):\n",
    "    f_M = np.random.uniform(0,1,loci_m)\n",
    "    G = np.random.binomial(n=2,p = f_M, size = (samples_n,loci_m))\n",
    "    G = preprocessing.scale(G, axis=0)\n",
    "    loci =random.randint(0,loci_m-1)\n",
    "    SNP = G[:,loci]\n",
    "    individuals = len(SNP)\n",
    "    mean = 0 \n",
    "    sigma_b = sqrt(var_g)\n",
    "    sigma_e = sqrt(var_e)\n",
    "    b_i = np.random.normal(0, sigma_b)\n",
    "    Y_n = np.zeros((individuals, 1))\n",
    "    for k in range(0, individuals):\n",
    "        #each individual will have a random e_j(noise) value\n",
    "        e_j = np.random.normal(0, sigma_e)\n",
    "        #G_ij will be the jth individual from our SNP for the loci of choce\n",
    "        G_ij  = SNP[k]\n",
    "        Y_j = b_i*G_ij + e_j\n",
    "        Y_n[k] = Y_j \n",
    "    #Before converting to categorical standerdize\n",
    "    Y_n = preprocessing.scale(Y_n, axis=0)\n",
    "    #Convert trait vector based on threshold here its point .2\n",
    "    #Y_n =  np.where(Y_n > 0.2, 1, 0)\n",
    "    #Alternative to threshold\n",
    "    Y_sorted = sorted(Y_n)\n",
    "    Ymed = np.median(Y_sorted);\n",
    "    Y_n =  np.where(Y_n > Ymed, 1, 0)\n",
    "    H= var_g/(var_g+var_e)\n",
    "    G = np.append(G, Y_n, axis=1)\n",
    "    return G,samples_n,loci_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Loci No Interaction Binary Traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trait_simulation_twolociBinary_nointer(samples_n, loci_m,var_g,var_e):\n",
    "    f_M = np.random.uniform(0,1,loci_m)\n",
    "    var_g = var_g/2\n",
    "    G = np.random.binomial(n=2,p = f_M, size = (samples_n,loci_m))\n",
    "    G = preprocessing.scale(G, axis=0)\n",
    "    #two random numbers without replacement\n",
    "    loci = random.sample(range(0,loci_m), 2)\n",
    "    SNP1 = G[:,loci[0]]\n",
    "    SNP2 = G[:,loci[1]]\n",
    "    #doesnt matter which SNP we choose from since their both same length\n",
    "    individuals = len(SNP1)\n",
    "    mean = 0 \n",
    "    sigma_b = sqrt(var_g)\n",
    "    sigma_e = sqrt(var_e)\n",
    "    b_i_SNP1 = np.random.normal(0, sigma_b)\n",
    "    b_i_SNP2 = np.random.normal(0, sigma_b)\n",
    "    Y_n = np.zeros((individuals, 1))\n",
    "    for k in range(0, individuals):\n",
    "        #each individual will have a random e_j(noise) value\n",
    "        e_j = np.random.normal(0, sigma_e)\n",
    "        #G1 and G2 will be the jth individual from our SNP1 and SNP 2 for the loci\n",
    "        G1 = SNP1[k]\n",
    "        G2 = SNP2[k]\n",
    "        Y_j = (G1*b_i_SNP1)+(G2*b_i_SNP2) + e_j\n",
    "        Y_n[k] = Y_j \n",
    "    Y_n = preprocessing.scale(Y_n, axis=0)\n",
    "    #Convert trait vector based on threshold here its point .2\n",
    "    #Y_n =  np.where(Y_n > 0.2, 1, 0)\n",
    "    #Alternative to threshold\n",
    "    Y_sorted = sorted(Y_n)\n",
    "    Ymed = np.median(Y_sorted);\n",
    "    Y_n =  np.where(Y_n > Ymed, 1, 0)\n",
    "    H= var_g/(var_g+var_e)\n",
    "    G = np.append(G, Y_n, axis=1)\n",
    "    return G,samples_n,loci_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Loci No Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trait_simulation_twoloci_nointer(samples_n, loci_m,var_g,var_e):\n",
    "    f_M = np.random.uniform(0,1,loci_m)\n",
    "    var_g = var_g/2\n",
    "    G = np.random.binomial(n=2,p = f_M, size = (samples_n,loci_m))\n",
    "    G = preprocessing.scale(G, axis=0)\n",
    "    #two random numbers without replacement\n",
    "    loci = random.sample(range(0,loci_m), 2)\n",
    "    SNP1 = G[:,loci[0]]\n",
    "    SNP2 = G[:,loci[1]]\n",
    "    print(SNP2[0:30])\n",
    "    #doesnt matter which SNP we choose from since their both same length\n",
    "    individuals = len(SNP1)\n",
    "    mean = 0 \n",
    "    sigma_b = sqrt(var_g)\n",
    "    sigma_e = sqrt(var_e)\n",
    "    b_i_SNP1 = np.random.normal(0, sigma_b)\n",
    "    b_i_SNP2 = np.random.normal(0, sigma_b)\n",
    "    Y_n = np.zeros((individuals, 1))\n",
    "    for k in range(0, individuals):\n",
    "        #each individual will have a random e_j(noise) value\n",
    "        e_j = np.random.normal(0, sigma_e)\n",
    "        #G1 and G2 will be the jth individual from our SNP1 and SNP 2 for the loci\n",
    "        G1 = SNP1[k]\n",
    "        G2 = SNP2[k]\n",
    "        Y_j = (G1*b_i_SNP1)+(G2*b_i_SNP2) + e_j\n",
    "        Y_n[k] = Y_j \n",
    "    H= var_g/(var_g+var_e)\n",
    "    G = np.append(G, Y_n, axis=1)\n",
    "    return G,samples_n,loci_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Loci  Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate base phenotype values\n",
    "def trait_simulation_twoloci_inter(samples_n,loci_m,var_g,var_e, n_causal_SNPs,b12_event,scalings = True):\n",
    "    #create allele frequencies\n",
    "    f_M = np.random.uniform(0,1,loci_m)\n",
    "    #create G matrix bases on allele frequencies\n",
    "    G = np.random.binomial(n=2,p = f_M, size = (samples_n,loci_m))\n",
    "    #scale or not depending on input, default is True\n",
    "    if scalings: \n",
    "        G = preprocessing.scale(G, axis=0)\n",
    "    #rows are the loci so each person has a row of different loci\n",
    "    individuals = len(G)    \n",
    "    sigma_e = sqrt(var_e)\n",
    "    sigma_b = sqrt(var_g/n_causal_SNPs)\n",
    "    #b_i = loci effect on phenotype\n",
    "    b_1 = np.random.normal(0, sigma_b)\n",
    "    b_2 = np.random.normal(0, sigma_b)\n",
    "    loci =random.sample(range(0, loci_m), 2)\n",
    "    print(loci[0])\n",
    "    print(loci[1])\n",
    "\n",
    "    SNP1 = G[:,loci[0]]\n",
    "    SNP2 = G[:,loci[1]]\n",
    "    individuals = len(SNP1)    \n",
    "    #rows are the loci so each person has a row of different loci\n",
    "    Y_n = np.zeros((individuals, 1));\n",
    "    \n",
    "    #depending on b1_event b12 will be different\n",
    "    # if 0 then b12 has no effect\n",
    "    if (b12_event == 0):\n",
    "        b_12 = 0\n",
    "    #if 1 then Random Combined Effect\n",
    "    elif(b12_event == 1):\n",
    "        b_12 = np.random.normal(0, sigma_b)\n",
    "    #if 2 then: 0 < b_12 < b1\n",
    "    elif(b12_event == 2):\n",
    "        b_12 = random.uniform(0, abs(b_1))\n",
    "    #if 3 then: 0 < b_12 < b2\n",
    "    elif(b12_event == 3):\n",
    "        b_12 = random.uniform(0, abs(b_2))\n",
    "    #if 4 then: b_1 + b_2 < b_12\n",
    "    elif(b12_event == 4):\n",
    "        b_12 = random.uniform(abs(b_1) + abs(b_2), 1)\n",
    "    #if 5 then: b_12 < 0\n",
    "    elif(b12_event == 5):\n",
    "        b_12 = random.uniform(-1 * sigma_b, 0)\n",
    "             \n",
    "    #create phenotype vector\n",
    "    for k in range(0, individuals):\n",
    "        #each individual will have a random e_j(noise) value\n",
    "        e_j = np.random.normal(0, sigma_e)\n",
    "        #G_ij will be the jth individual from our SNP for the loci of choice\n",
    "        G_ij1  = SNP1[k]\n",
    "        G_ij2  = SNP2[k]\n",
    "        Y_j = (b_1*G_ij1) + (b_2*G_ij2) + (b_12 * (G_ij1 * G_ij2))+ e_j \n",
    "        Y_n[k] = Y_j \n",
    "    #add Y traits to G matrix\n",
    "    G = np.append(G, Y_n, axis=1)\n",
    "    return G, loci_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Genotype\n",
    "def generate_genotype(n, m, processing = True):\n",
    "    f_M = np.random.uniform(0,1,m)\n",
    "    G = np.random.binomial(n=2,p = f_M, size =  (n,m))\n",
    "    if processing: \n",
    "        G = preprocessing.scale(G, axis=0)\n",
    "    return G\n",
    "\n",
    "#Generate base phenotype values\n",
    "def generate_base_pheno_values(G, var_g, var_e, n_causal_SNPs):\n",
    "    #rows are the loci so each person has a row of different loci\n",
    "    individuals = len(G)    \n",
    "    sigma_e = sqrt(var_e)\n",
    "    sigma_b = sqrt(var_g/n_causal_SNPs)\n",
    "    #b_i = loci effect on phenotype\n",
    "    b_1 = np.random.normal(0, sigma_b)\n",
    "    b_2 = np.random.normal(0, sigma_b)\n",
    "    \n",
    "    loci =random.sample(range(0, len(G[0])), 2)\n",
    "    SNP1 = G[:,loci[0]]\n",
    "    SNP2 = G[:,loci[1]]\n",
    "    return b_1, b_2, SNP1, SNP2, sigma_e, sigma_b\n",
    "\n",
    "def generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12):\n",
    "    individuals = len(SNP1)    \n",
    "    #rows are the loci so each person has a row of different loci\n",
    "    Y_n = np.zeros((individuals, 1));\n",
    "    for k in range(0, individuals):\n",
    "        #each individual will have a random e_j(noise) value\n",
    "#         e_j = np.random.normal(0, sigma_e)\n",
    "        e_j =0\n",
    "        \n",
    "        #G_ij will be the jth individual from our SNP for the loci of choice\n",
    "        G_ij1  = SNP1[k]\n",
    "        G_ij2  = SNP2[k]\n",
    "        Y_j = (b_1*G_ij1) + (b_2*G_ij2) + (b_12 *( G_ij1 * G_ij2))+ e_j \n",
    "        Y_n[k] = Y_j \n",
    "    return Y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_interact_avg(shap_values,x_test,forReg):\n",
    "    shap_values = np.array(shap_values)\n",
    "    avg_shap = []\n",
    "    for i in range(0,len(shap_values[0])):\n",
    "        shap2 = np.mean(abs(shap_values[:,i]))\n",
    "        avg_shap.append(shap2)\n",
    "    temp1 = np.asarray(avg_shap)     \n",
    "    indices = temp1.argsort()[-4:][::-1]\n",
    "    index1 = int(indices[0])\n",
    "    index2 = int(indices[1])\n",
    "    shap_interaction_values = shap.TreeExplainer(forReg).shap_interaction_values(x_test)\n",
    "    shap_avg = np.mean(shap_interaction_values[:,index1,index2])\n",
    "    return shap_avg,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ints(shap_interaction_values, indices,shap_avgL):\n",
    "    loci_plots = np.zeros((len(shap_interaction_values),6))\n",
    "    loci_plots[:,0] = shap_interaction_values[:,indices[0],indices[1]]\n",
    "    loci_plots[:,1] = shap_interaction_values[:,indices[0],indices[2]]\n",
    "    loci_plots[:,2] = shap_interaction_values[:,indices[0],indices[3]]\n",
    "    loci_plots[:,3] = shap_interaction_values[:,indices[1],indices[2]]\n",
    "    loci_plots[:,4] = shap_interaction_values[:,indices[1],indices[3]]\n",
    "    loci_plots[:,5] = shap_interaction_values[:,indices[2],indices[3]]\n",
    "    data_to_plot =loci_plots\n",
    "    positions = np.arange(6) + 1\n",
    "    fig, ax = plt.subplots(1,1, figsize=(9,4))\n",
    "    loci_pairs = ['Loci '+str(indices[0])+'&'+str(indices[1]), 'Loci '+str(indices[0])+'&'+str(indices[2]), \n",
    "                  'Loci '+str(indices[0])+'&'+str(indices[3]), \n",
    "                  'Loci '+str(indices[1])+'&'+str(indices[2]), \n",
    "                  'Loci '+str(indices[1])+'&'+str(indices[3]),'Loci '+str(indices[2])+'&'+str(indices[3])]\n",
    "    bp = ax.boxplot(data_to_plot, positions=positions,showfliers=False,whis=None,whiskerprops=None)\n",
    "    means = [np.mean(data) for data in data_to_plot.T]\n",
    "    ax.plot(positions, means, 'rs')\n",
    "    ax.set_xticklabels(loci_pairs, fontdict=None, minor=False)\n",
    "    ax.set_title(\"Average SHAP Interaction Between Loci, e = 0\")\n",
    "    ax.set_ylabel(\"SHAP Interaction Value\")\n",
    "    ax.set_xlabel(\"Loci Pairs\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generate_genotype(1000, 50)\n",
    "b_1, b_2, SNP1, SNP2, sigma_e, sigma_b = generate_base_pheno_values(G, 0.9, .2, 2)\n",
    "b_1 = .5\n",
    "b_2 = .5\n",
    "\n",
    "Y_lists = []\n",
    "#No Effect\n",
    "b_12 = 0\n",
    "Z = generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12)\n",
    "Y_lists.append(Z)\n",
    "\n",
    "#Random Combined Effect\n",
    "# b_12 = np.random.normal(0, sigma_b)\n",
    "b_12 = 2\n",
    "A = generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12)\n",
    "Y_lists.append(A)\n",
    "\n",
    "#0 < b_12 < b1\n",
    "# b_12 = random.uniform(0, abs(b_1))\n",
    "b_12 = -2\n",
    "B = generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12)\n",
    "Y_lists.append(B)\n",
    "\n",
    "#0 < b_12 < b2\n",
    "# b_12 = random.uniform(0, abs(b_2))\n",
    "b_12 = 4\n",
    "C = generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12)\n",
    "Y_lists.append(C)\n",
    "\n",
    "\n",
    "#b_1 + b_2 < b_12\n",
    "# b_12 = random.uniform(abs(b_1) + abs(b_2), 1)\n",
    "b_12 = -4\n",
    "D = generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12)\n",
    "Y_lists.append(D)\n",
    "\n",
    "#b_12 < 0\n",
    "# b_12 = random.uniform(-1 * sigma_b, 0)\n",
    "# E = generate_phenotype_additive(b_1, b_2, SNP1, SNP2, sigma_e, b_12)\n",
    "# Y_lists.append(E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G0 to G5 are for b12 values above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0 = np.append(G, Y_lists[0], axis=1)\n",
    "G1 = np.append(G, Y_lists[1], axis=1)\n",
    "G2 = np.append(G, Y_lists[2], axis=1)\n",
    "G3 = np.append(G, Y_lists[3], axis=1)\n",
    "G4 = np.append(G, Y_lists[4], axis=1)\n",
    "# G5 = np.append(G, Y_lists[5], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G_oneloci is for one loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_oneloci, samples_n,loci_m = trait_simulation_oneloci(1000,20,.8,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G_oneloci_binary is for one loci, this has binary traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_oneloci_binary, samples_n,loci_m = trait_simulation_oneloci_binary(500,20,.7,.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G_twoloci_binary is for one loci, this has binary traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_twoloci_binary, samples_n,loci_m = trait_simulation_twolociBinary_nointer(500,20,.7,.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G_twoloci is for two loci no interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_twoloci, samples_n,loci_m = trait_simulation_twoloci_nointer(1000,40,.5,.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G_twointerloci is for two loci with interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = [0,.1,.2,.3,.4,.5,.6,.7,.8,.9]\n",
    "[float(m) for m in e_list]# #this is a list of list\n",
    "var_e = e_list[randrange(10)]\n",
    "G_twointerloci,loci_m = trait_simulation_twoloci_inter(1000,40,.5,var_e,2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F_i distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use this to see distribution of f_i frequency\n",
    "# bin_size = 20\n",
    "# count, bins, ignored = plt.hist(f_M, 20, facecolor='blue') \n",
    "\n",
    "# plt.xlabel('X [0,1]')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title(\"Uniform Distribution For Loci Frequency Bin size: \"+str(bin_size))\n",
    "# plt.axis([0, 1, 0, len(f_M]) # x_start, x_end, y_start, y_end\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.show(block = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into test and train\n",
    "X = G_oneloci[:,0:len(G_oneloci[0])-1]\n",
    "\n",
    "#last column is the appended Y vector we predicted\n",
    "y = G_oneloci[:,len(G_oneloci[0])-1]\n",
    "\n",
    "#split the data, 70% training\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the algorithm\n",
    "linReg = LinearRegression() \n",
    "linReg.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "est = sm.OLS(y, X)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = est2.pvalues[:]\n",
    "p_vals = np.array(p_vals)\n",
    "p_vals = p_vals.reshape(1,len(p_vals))\n",
    "beta_vals = p_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = beta_vals.argsort()[-2:][::-1]\n",
    "avg_beta_loci1 = beta_vals[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(beta_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape(1,len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmax(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on test data\n",
    "y_predict = linReg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = abs(linReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = temp.argsort()[-2:][::-1]\n",
    "index1 = int(indices[0])\n",
    "avg_beta_loci1 = temp[index1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression model\n",
    "print(\"Mean absolute error =\", round(metrics.mean_absolute_error(y_test, y_predict), 2))\n",
    "print(\"Mean squared error =\", round(metrics.mean_squared_error(y_test, y_predict), 2)) \n",
    "rmse = metrics.mean_squared_error(y_test, y_predict)\n",
    "print(\"Root Mean squared error =\", round(sqrt(rmse),2))\n",
    "print(\"Median absolute error =\", round(metrics.median_absolute_error(y_test, y_predict), 2)) \n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_predict), 2)) \n",
    "print(\"R2 score =\", round(metrics.r2_score(y_test, y_predict), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction error plot\n",
    "diff = y_test - y_predict\n",
    "plt.hist(diff,bins = 30, color = 'blue')\n",
    "plt.title('Prediction Errors')\n",
    "plt.xlabel('Phenotype prediction error')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = []\n",
    "for i in range(0,19):\n",
    "    x_vals.append(x_train[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "index, value = max(enumerate(x_vals), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(linReg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.std(x_train, 0)*linReg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "explainer = shap.KernelExplainer(linReg.predict, shap.sample(x_train,100))\n",
    "shap_values = explainer.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = np.array(shap_values)\n",
    "avg_shap = []\n",
    "for i in range(0,len(shap_values[0])):\n",
    "    shap2 = np.mean(abs(shap_values[:,i]))\n",
    "    avg_shap.append(shap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_vals = avg_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casual_loci_index=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "for i in range (0, len(shap_vals)):\n",
    "        #TP\n",
    "    if(i == casual_loci_index and shap_vals[i] >= threshold):\n",
    "        TP = TP+1\n",
    "        #FN\n",
    "    elif(i == casual_loci_index and shap_vals[i] < threshold):\n",
    "        FN =  FN+1\n",
    "        #TN\n",
    "    elif(i != casual_loci_index and shap_vals[i] < threshold):\n",
    "        TN = TN+1\n",
    "        #FP\n",
    "    elif(i != casual_loci_index and shap_vals[i] >= threshold):\n",
    "        FP = FP+1\n",
    "    else:\n",
    "        a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TPR is: '+str(TPR)+' FPR is: '+str(FPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regression\n",
    "#initialize tree with a node depth of 10 and 50 decision trees\n",
    "forReg = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=50)\n",
    "forReg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict2 = forReg.predict(x_test)\n",
    "#Evaluate the regression model\n",
    "print(\"Mean absolute error =\", round(metrics.mean_absolute_error(y_test, y_predict2), 2))\n",
    "print(\"Mean squared error =\", round(metrics.mean_squared_error(y_test, y_predict2), 2)) \n",
    "rmse = metrics.mean_squared_error(y_test, y_predict2)\n",
    "print(\"Root Mean squared error =\", round(sqrt(rmse),2))\n",
    "print(\"Median absolute error =\", round(metrics.median_absolute_error(y_test, y_predict2), 2)) \n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_predict2), 2)) \n",
    "print(\"R2 score =\", round(metrics.r2_score(y_test, y_predict2), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFR Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = y_test - y_predict2\n",
    "plt.hist(diff,bins = 30, color = 'blue')\n",
    "plt.title('Prediction Errors')\n",
    "plt.xlabel('Phenotype prediction error')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFR SHAP Values and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "shap_values = shap.TreeExplainer(forReg).shap_values(x_train)\n",
    "# shap_values = shap.TreeExplainer(forReg).shap_values(x_train)\n",
    "shap.summary_plot(shap_values, x_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_shap_values(shap_values):\n",
    "#     if len(shap_values) == 2:\n",
    "#         mean_shap = [0] * len(shap_values[0][0])\n",
    "#         for shap_value in shap_values[0]:\n",
    "#             for x in range(0, len(shap_value)):\n",
    "#                 mean_shap[x] = mean_shap[x] + abs(shap_value[x])\n",
    "#         for x in range(0, len(shap_values[0])):\n",
    "#             mean_shap[x] = abs(mean_shap[x] / len(shap_values[0]))\n",
    "#     else:    \n",
    "#         mean_shap = [0] * len(shap_values[0])\n",
    "#         for shap_value in shap_values:\n",
    "#             for x in range(0, len(shap_value)):\n",
    "#                 mean_shap[x] = mean_shap[x] + abs(shap_value[x])\n",
    "#         for x in range(0, len(shap_values[0])):\n",
    "#             mean_shap[x] = abs(mean_shap[x] / len(shap_values[0]))\n",
    "#     return mean_shap                   \n",
    "\n",
    "# def max_mean_feature(shap_values):\n",
    "#     mean_shap = mean_shap_values(shap_values)\n",
    "#     return mean_shap.index(max(mean_shap)), mean_shap[mean_shap.index(max(mean_shap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index, shap = max_mean_feature(shap_values)\n",
    "# shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shap = []\n",
    "for i in range(0,len(shap_values[0])):\n",
    "    shap2 = np.mean(abs(shap_values[:,i]))\n",
    "    avg_shap.append(shap2)\n",
    "temp1 = np.asarray(avg_shap)     \n",
    "indices = temp1.argsort()[-1:][::-1]\n",
    "# loci1,loci2 = avg_shap[indices[0]],avg_shap[indices[1]]\n",
    "int(indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Interaction Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap_interaction_values = shap.TreeExplainer(forReg).shap_interaction_values(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shap_interaction_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.summary_plot(shap_interaction_values, features=x_test, max_display=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_avg,indices = shap_interact_avg(shap_values,x_test,forReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHAP_AVG_List(shap_interaction_values,indices):\n",
    "    shap_avgL = []\n",
    "    shap_avg = np.mean(shap_interaction_values[:,indices[0],indices[1]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.mean(shap_interaction_values[:,indices[0],indices[2]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.mean(shap_interaction_values[:,indices[0],indices[3]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.mean(shap_interaction_values[:,indices[1],indices[2]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.mean(shap_interaction_values[:,indices[1],indices[3]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.mean(shap_interaction_values[:,indices[2],indices[3]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_interaction = shap_avgL\n",
    "    shap_interaction2 = np.asarray(shap_interaction).reshape((1, 6))\n",
    "    avg_shap_inter = pd.DataFrame(shap_interaction2)\n",
    "    avg_shap_inter.columns = ['Loci '+str(indices[0])+'&'+str(indices[1]), 'Loci '+str(indices[0])+'&'+str(indices[2]), \n",
    "                  'Loci '+str(indices[0])+'&'+str(indices[3]), \n",
    "                  'Loci '+str(indices[1])+'&'+str(indices[2]), \n",
    "                  'Loci '+str(indices[1])+'&'+str(indices[3]),'Loci '+str(indices[2])+'&'+str(indices[3])]\n",
    "    return avg_shap_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHAP_Median_List(shap_interaction_values,indices):\n",
    "    shap_avgL = []\n",
    "    shap_avg = np.median(shap_interaction_values[:,indices[0],indices[1]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.median(shap_interaction_values[:,indices[0],indices[2]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.median(shap_interaction_values[:,indices[0],indices[3]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.median(shap_interaction_values[:,indices[1],indices[2]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.median(shap_interaction_values[:,indices[1],indices[3]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_avg = np.median(shap_interaction_values[:,indices[2],indices[3]])\n",
    "\n",
    "    shap_avgL.append(shap_avg)\n",
    "    shap_interaction = shap_avgL\n",
    "    shap_interaction2 = np.asarray(shap_interaction).reshape((1, 6))\n",
    "    avg_shap_inter = pd.DataFrame(shap_interaction2)\n",
    "    avg_shap_inter.columns = ['Loci '+str(indices[0])+'&'+str(indices[1]), 'Loci '+str(indices[0])+'&'+str(indices[2]), \n",
    "                  'Loci '+str(indices[0])+'&'+str(indices[3]), \n",
    "                  'Loci '+str(indices[1])+'&'+str(indices[2]), \n",
    "                  'Loci '+str(indices[1])+'&'+str(indices[3]),'Loci '+str(indices[2])+'&'+str(indices[3])]\n",
    "    return avg_shap_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_avgL = SHAP_AVG_List(shap_interaction_values,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_medianL = SHAP_Median_List(shap_interaction_values,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_interaction_values, features=x_test, max_display=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shap_inter2 = plot_ints(shap_interaction_values,indices,shap_avgL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_avgL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_medianL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"compact_dot\" is only used for SHAP interaction values.\n",
    "shap.summary_plot(shap_interaction_values, features=x_test, max_display=5,plot_type=\"compact_dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = shap_interaction_values.shape\n",
    "shap_interaction_values_2d = np.reshape(np.ravel(shap_interaction_values), (dim[0], dim[1]*dim[2]))\n",
    "\n",
    "# make all pairs of features\n",
    "x1 = pd.DataFrame(x_test)\n",
    "\n",
    "# make all pairs of features\n",
    "x2 = x1[np.repeat(x1.columns.tolist(), len(x1.columns))]\n",
    "x2.columns =  [str(i)+\":\"+str(j) for i in x1.columns for j in x1.columns]\n",
    "\n",
    "shap.summary_plot(shap_interaction_values_2d, x2, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_logreg = LogisticRegression()\n",
    "my_logreg.fit(x_train, y_train)\n",
    "y_predict_lr = my_logreg.predict(x_test)\n",
    "score_lr = accuracy_score(y_test, y_predict_lr)\n",
    "print(\"Accuracy for Logistic Regressions was: \"+ str(score_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "explainer = shap.KernelExplainer(my_logreg.predict, shap.sample(x_train,50))\n",
    "shap_values = explainer.shap_values(x_train)\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow tensorflow-probability\n",
    "model = Sequential()\n",
    "hidden_nodes = len(X[0])+20 \n",
    "model.add(Dense( 256, activation='relu', input_dim = 32))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='relu', input_dim = 256))\n",
    "model.add(Dropout(0.25))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "# model.fit(x_train, y_train, epochs=50, batch_size=16, verbose = 0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam',activation='relu',neurons=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense( neurons, activation=activation, input_dim = 20))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation=activation, input_dim = neurons))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "    return model\n",
    "    # model.fit(x_train, y_train, epochs=50, batch_size=16, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 25, 50]\n",
    "neurons = [10, 32, 64, 128, 256]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(epochs=epochs,batch_size=batch_size,optimizer=optimizer,activation=activation,neurons =neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10,scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/kevin/Downloads/GridSearchCv_Initial', 'wb') as fp:\n",
    "    pickle.dump(grid_result.best_params_, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model, x_train)\n",
    "shap_values = explainer.shap_values(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values = np.array(shap_values)\n",
    "# avg_shap = []\n",
    "# for i in range(0,len(shap_values[0])):\n",
    "#     shap2 = np.mean(abs(shap_values[:,i]))\n",
    "#     avg_shap.append(shap2)\n",
    "# temp1 = np.asarray(avg_shap)\n",
    "# indices = temp1.argsort()[-2:][::-1]\n",
    "# loci1,loci2 = avg_shap[indices[0]],avg_shap[indices[1]]\n",
    "# indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim = len(X[0])))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(1, activation='relu', input_dim = 32))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.compile(loss='mean_absolute_error', optimizer='rmsprop')\n",
    "# model.fit(x_train, y_train, epochs=50, batch_size=16, verbose = 0)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('NN Accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression model\n",
    "print(\"Mean absolute error =\", round(metrics.mean_absolute_error(y_test, y_predict), 2))\n",
    "print(\"Mean squared error =\", round(metrics.mean_squared_error(y_test, y_predict), 2)) \n",
    "rmse = metrics.mean_squared_error(y_test, y_predict)\n",
    "print(\"Root Mean squared error =\", round(sqrt(rmse),2))\n",
    "print(\"Median absolute error =\", round(metrics.median_absolute_error(y_test, y_predict), 2)) \n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_predict), 2)) \n",
    "print(\"R2 score =\", round(metrics.r2_score(y_test, y_predict), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "y_predict = y_predict.reshape(len(y_predict))\n",
    "\n",
    "diff = y_test - y_predict\n",
    "plt.hist(diff,bins = 30, color = 'blue')\n",
    "plt.title('Prediction Errors')\n",
    "plt.xlabel('Phenotype prediction error')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half =  round((loci_m-1)/2)\n",
    "model = Sequential()\n",
    "model.add(Dense(half, activation='relu', input_dim = loci_m-1))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='relu', input_dim = half))\n",
    "model.add(Dropout(0.25))\n",
    "model.compile(loss='mean_absolute_error', optimizer='rmsprop')\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=16, verbose = 0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half =  round((loci_m-1)/2)\n",
    "model = Sequential()\n",
    "model.add(Dense(half, activation='relu', input_dim = loci_m-1))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid', input_dim = half))\n",
    "model.add(Dropout(0.25))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=16, verbose = 0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NN Accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the regression model\n",
    "print(\"Mean absolute error =\", round(metrics.mean_absolute_error(y_test, y_predict), 2))\n",
    "print(\"Mean squared error =\", round(metrics.mean_squared_error(y_test, y_predict), 2)) \n",
    "rmse = metrics.mean_squared_error(y_test, y_predict)\n",
    "print(\"Root Mean squared error =\", round(sqrt(rmse),2))\n",
    "print(\"Median absolute error =\", round(metrics.median_absolute_error(y_test, y_predict), 2)) \n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_predict), 2)) \n",
    "print(\"R2 score =\", round(metrics.r2_score(y_test, y_predict), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "y_predict = y_predict.reshape(len(y_predict))\n",
    "\n",
    "diff = y_test - y_predict\n",
    "plt.hist(diff,bins = 30, color = 'blue')\n",
    "plt.title('Prediction Errors')\n",
    "plt.xlabel('Phenotype prediction error')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "explainer = shap.DeepExplainer(model, x_train)\n",
    "shap_values = explainer.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoReg = linear_model.Lasso(alpha=.1)\n",
    "lassoReg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = lassoReg.coef_\n",
    "coeffs = abs(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lassoReg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute error =\", round(metrics.mean_absolute_error(y_test, y_predict), 2))\n",
    "print(\"Mean squared error =\", round(metrics.mean_squared_error(y_test, y_predict), 2)) \n",
    "rmse = metrics.mean_squared_error(y_test, y_predict)\n",
    "print(\"Root Mean squared error =\", round(sqrt(rmse),2))\n",
    "print(\"Median absolute error =\", round(metrics.median_absolute_error(y_test, y_predict), 2)) \n",
    "print(\"Explain variance score =\", round(metrics.explained_variance_score(y_test, y_predict), 2)) \n",
    "print(\"R2 score =\", round(metrics.r2_score(y_test, y_predict), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
